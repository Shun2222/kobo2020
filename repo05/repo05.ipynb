{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レポート課題５\n",
    "## 1910094 植木 駿介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  \n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 5 #20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.298921476786026\n",
      "=== epoch:1, train acc:0.183, test acc:0.151 ===\n",
      "train loss:2.296901885297352\n",
      "train loss:2.290696909439824\n",
      "train loss:2.2881882799615747\n",
      "train loss:2.276505338605166\n",
      "train loss:2.254780373810562\n",
      "train loss:2.2511367401782674\n",
      "train loss:2.2312041499775903\n",
      "train loss:2.2011010496446364\n",
      "train loss:2.182002840225326\n",
      "train loss:2.163528114147389\n",
      "train loss:2.100616348432205\n",
      "train loss:2.0676888032706566\n",
      "train loss:2.0205938596435344\n",
      "train loss:1.9622616964455153\n",
      "train loss:1.9036973071878314\n",
      "train loss:1.8179408672391846\n",
      "train loss:1.7248969445052904\n",
      "train loss:1.596421223111243\n",
      "train loss:1.6176229732557377\n",
      "train loss:1.473987843887752\n",
      "train loss:1.3476412363876171\n",
      "train loss:1.2829792301042007\n",
      "train loss:1.232851461577629\n",
      "train loss:1.2394264807183868\n",
      "train loss:1.1210770884993457\n",
      "train loss:1.0778868464187386\n",
      "train loss:0.8280650543788843\n",
      "train loss:0.8250310671946433\n",
      "train loss:0.7695084205481676\n",
      "train loss:0.7804765627618024\n",
      "train loss:0.778958751466411\n",
      "train loss:0.7746911501717285\n",
      "train loss:0.7175749704588885\n",
      "train loss:0.6175094146208615\n",
      "train loss:0.8423477820606784\n",
      "train loss:0.6383165547176459\n",
      "train loss:0.5284003319366185\n",
      "train loss:0.6097922482658443\n",
      "train loss:0.46692281036680705\n",
      "train loss:0.5364849541541242\n",
      "train loss:0.6231083373660046\n",
      "train loss:0.613846862017852\n",
      "train loss:0.4561775501712557\n",
      "train loss:0.6517434913234748\n",
      "train loss:0.3879415752033524\n",
      "train loss:0.6550342520297481\n",
      "train loss:0.4312638901965702\n",
      "train loss:0.3705597239379694\n",
      "train loss:0.6156653812132904\n",
      "train loss:0.523401824301336\n",
      "=== epoch:2, train acc:0.835, test acc:0.828 ===\n",
      "train loss:0.5635936059191216\n",
      "train loss:0.5109003834400313\n",
      "train loss:0.5102586605318401\n",
      "train loss:0.5001048385323965\n",
      "train loss:0.43290266482264855\n",
      "train loss:0.597238519490063\n",
      "train loss:0.3664992110350794\n",
      "train loss:0.3384865852064916\n",
      "train loss:0.4221400482763615\n",
      "train loss:0.4706637928574786\n",
      "train loss:0.428831669518583\n",
      "train loss:0.373247215648419\n",
      "train loss:0.489231338020967\n",
      "train loss:0.27122200431041976\n",
      "train loss:0.48558283039852684\n",
      "train loss:0.29827993598635055\n",
      "train loss:0.37411497762957263\n",
      "train loss:0.3411079910298344\n",
      "train loss:0.3357754586822128\n",
      "train loss:0.3356523930431072\n",
      "train loss:0.47817437757913667\n",
      "train loss:0.36219663053284384\n",
      "train loss:0.3128474534894381\n",
      "train loss:0.33346728254846947\n",
      "train loss:0.4141175525881801\n",
      "train loss:0.4295967832720191\n",
      "train loss:0.36814083788931024\n",
      "train loss:0.4452777631084551\n",
      "train loss:0.3470839935807512\n",
      "train loss:0.25192397030519337\n",
      "train loss:0.4635268833298294\n",
      "train loss:0.2816709900942998\n",
      "train loss:0.3664332763307457\n",
      "train loss:0.28661336501730394\n",
      "train loss:0.31978190576563664\n",
      "train loss:0.3412791601613368\n",
      "train loss:0.4666695319353735\n",
      "train loss:0.41636360194539895\n",
      "train loss:0.41393849931250826\n",
      "train loss:0.29828358561029517\n",
      "train loss:0.22835210844930898\n",
      "train loss:0.3899124495254223\n",
      "train loss:0.31467474029773235\n",
      "train loss:0.28524580644079084\n",
      "train loss:0.282265120122727\n",
      "train loss:0.20982355655537632\n",
      "train loss:0.2619360733948042\n",
      "train loss:0.4134445237712345\n",
      "train loss:0.3620711611456335\n",
      "train loss:0.44071211427624596\n",
      "=== epoch:3, train acc:0.877, test acc:0.873 ===\n",
      "train loss:0.29985538698315517\n",
      "train loss:0.4134856668142465\n",
      "train loss:0.4648464559881951\n",
      "train loss:0.3597542475931328\n",
      "train loss:0.4001970231993027\n",
      "train loss:0.27624260801400163\n",
      "train loss:0.359861769699261\n",
      "train loss:0.23332388547291572\n",
      "train loss:0.3444957970963172\n",
      "train loss:0.40957949893375445\n",
      "train loss:0.3771089348062096\n",
      "train loss:0.25803910692321375\n",
      "train loss:0.31403123743064304\n",
      "train loss:0.3600262883234292\n",
      "train loss:0.3845305505166216\n",
      "train loss:0.2776584981188262\n",
      "train loss:0.5355634235505412\n",
      "train loss:0.4305318734169567\n",
      "train loss:0.25863248625208296\n",
      "train loss:0.19652189448393453\n",
      "train loss:0.2225385074985618\n",
      "train loss:0.19299114130762604\n",
      "train loss:0.3289301920471496\n",
      "train loss:0.2881335178912375\n",
      "train loss:0.3443475061501834\n",
      "train loss:0.26120702344265195\n",
      "train loss:0.4291442426542165\n",
      "train loss:0.3281723752039118\n",
      "train loss:0.37893335359911684\n",
      "train loss:0.3574196249973108\n",
      "train loss:0.3295073810415425\n",
      "train loss:0.26340001222263765\n",
      "train loss:0.31306477031719915\n",
      "train loss:0.23599518527651786\n",
      "train loss:0.2868653682062621\n",
      "train loss:0.20557991024812644\n",
      "train loss:0.27659591280137535\n",
      "train loss:0.28025192712753155\n",
      "train loss:0.12651822709928937\n",
      "train loss:0.1917529753448235\n",
      "train loss:0.29543795176406495\n",
      "train loss:0.2269632221944985\n",
      "train loss:0.2631940817619014\n",
      "train loss:0.2814059675037689\n",
      "train loss:0.2709005546257239\n",
      "train loss:0.19474697122422996\n",
      "train loss:0.3730607456269286\n",
      "train loss:0.2777395231844319\n",
      "train loss:0.2568706916399097\n",
      "train loss:0.14896565653609314\n",
      "=== epoch:4, train acc:0.892, test acc:0.889 ===\n",
      "train loss:0.31232228840654935\n",
      "train loss:0.21521876115413574\n",
      "train loss:0.35003544582596513\n",
      "train loss:0.3956584527181346\n",
      "train loss:0.22732786129660298\n",
      "train loss:0.2729136862977715\n",
      "train loss:0.13432341641151843\n",
      "train loss:0.27619024016978416\n",
      "train loss:0.2860835194173696\n",
      "train loss:0.13190264135321608\n",
      "train loss:0.16184162448381711\n",
      "train loss:0.15882541431759503\n",
      "train loss:0.1829017360956937\n",
      "train loss:0.2184749330376721\n",
      "train loss:0.14439244556722847\n",
      "train loss:0.18894112469698943\n",
      "train loss:0.2548756055207582\n",
      "train loss:0.2313336317080155\n",
      "train loss:0.3039516833152987\n",
      "train loss:0.20051159796442408\n",
      "train loss:0.10174649761853864\n",
      "train loss:0.24586892630810442\n",
      "train loss:0.29021543892616714\n",
      "train loss:0.1748216428495889\n",
      "train loss:0.3310006793250531\n",
      "train loss:0.23386179662065026\n",
      "train loss:0.35842477147817225\n",
      "train loss:0.1719292336838005\n",
      "train loss:0.12485503580953564\n",
      "train loss:0.18729842913299863\n",
      "train loss:0.16844535431585966\n",
      "train loss:0.17234221688893864\n",
      "train loss:0.14391854648680918\n",
      "train loss:0.20814965260515422\n",
      "train loss:0.20090181950888528\n",
      "train loss:0.3069607868622624\n",
      "train loss:0.19131598786129855\n",
      "train loss:0.12965523448709274\n",
      "train loss:0.2074304461636517\n",
      "train loss:0.40632284156318993\n",
      "train loss:0.1517946651176541\n",
      "train loss:0.2749055257830387\n",
      "train loss:0.10933933857716799\n",
      "train loss:0.21945412324977237\n",
      "train loss:0.36071621724848946\n",
      "train loss:0.1415994665886565\n",
      "train loss:0.27609066247044023\n",
      "train loss:0.20993357778575658\n",
      "train loss:0.27986630409467905\n",
      "train loss:0.16681919965652894\n",
      "=== epoch:5, train acc:0.93, test acc:0.898 ===\n",
      "train loss:0.22427534596503804\n",
      "train loss:0.1863869855486834\n",
      "train loss:0.17892975371986217\n",
      "train loss:0.19427368589626468\n",
      "train loss:0.15230791153271334\n",
      "train loss:0.1965136979005451\n",
      "train loss:0.16471047814464634\n",
      "train loss:0.10694051685162247\n",
      "train loss:0.1705033378665044\n",
      "train loss:0.19780095608200157\n",
      "train loss:0.09273436605253021\n",
      "train loss:0.21322969297424696\n",
      "train loss:0.14431390918815615\n",
      "train loss:0.10760683330930837\n",
      "train loss:0.20663859117765146\n",
      "train loss:0.1715688873260016\n",
      "train loss:0.16578301906550813\n",
      "train loss:0.2874389996964049\n",
      "train loss:0.21876732561079057\n",
      "train loss:0.09570535215878062\n",
      "train loss:0.15306024374323365\n",
      "train loss:0.17499666424344892\n",
      "train loss:0.2230665143817661\n",
      "train loss:0.12985521646680082\n",
      "train loss:0.13988587964978563\n",
      "train loss:0.12558451965495507\n",
      "train loss:0.19448433407240556\n",
      "train loss:0.21074918266215975\n",
      "train loss:0.32747652221146256\n",
      "train loss:0.13195743794988885\n",
      "train loss:0.18565228400880215\n",
      "train loss:0.14358203467246095\n",
      "train loss:0.19994696651626787\n",
      "train loss:0.1030518181569836\n",
      "train loss:0.10027689282188529\n",
      "train loss:0.21093448345736873\n",
      "train loss:0.13981587013566638\n",
      "train loss:0.20339593572835535\n",
      "train loss:0.21728057780193094\n",
      "train loss:0.28210519274205653\n",
      "train loss:0.146533919854963\n",
      "train loss:0.24993469691729486\n",
      "train loss:0.4095204697787051\n",
      "train loss:0.08892518019359415\n",
      "train loss:0.10479677078479116\n",
      "train loss:0.3188454229243903\n",
      "train loss:0.20392910886501017\n",
      "train loss:0.40496545064519496\n",
      "train loss:0.16725068576162133\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.919\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU1f3/8dcn+wYJ+xZWRRBXFBEXFLEK2Faxi1Wr7bcb1Wpr/Soq3fvrt9+iWGutKKWWar/uVVyqyKKCaBUFFBeWsCkSwhICCWSZLJPz+2MGmIQJTCA3N5l5Px+PeeQu5879zFXO555z7z3XnHOIiEjiSvI7ABER8ZcSgYhIglMiEBFJcEoEIiIJTolARCTBKRGIiCQ4zxKBmc0ysx1m9kkT683M7jOz9Wb2kZmd5lUsIiLSNC9bBA8D4w+xfgIwOPyZBDzoYSwiItIEzxKBc24xsOsQRS4D/ulClgB5ZtbLq3hERCS6FB/33QfYHDFfGF62tXFBM5tEqNVAdnb26UOHDm2VAEVE4sXy5ct3Oue6RVvnZyKwKMuijnfhnJsJzAQYMWKEW7ZsmZdxiYjEHTPb1NQ6P+8aKgT6RsznA0U+xSIikrD8TAQvAt8K3z00Cihzzh3ULSQiIt7yrGvIzJ4AxgBdzawQ+DWQCuCcmwHMAS4B1gOVwHe8ikVERJrmWSJwzl11mPUOuMGr/YuISGz0ZLGISIJTIhARSXBKBCIiCU6JQEQkwSkRiIgkOCUCEZEEp0QgIpLg/BxrSEREYvD8B1uYNq+AotIqeudlMnncECYO79Ni369EICLShj3/wRbumP0Rgdp6ALaUVjFl9scALZYMlAhERHzinGNvdR3bygJsLQuwrayKrWUBtu/ZNx+gYPteXKNxmatqg0ybV6BEICLSljnn2FVRs79C37onwPZ9Ff6eqv3LK2uCB23bNSednrnp5HfKYs22vVG/v6i0qsViVSIQEWmmYL2jeG81W8uq2FYWYNueQMRZfYCte6rYXlZNTbC+wXbJSUaPDun0yM1gaM8OjDmuO71yM+iRm0Gv3Ax6dsygR8cM0lIO3MdzztTX2RKl0u+dl9liv0eJQEQkQnVdkB17qtlaFmBrWVWDbpp9f4vLqwnWN+yvSUtJomfHDHrmZnBav070zM2gV3i+Z24mvXIz6JqTTnJStHdyNW3yuCFMmf0xVbUHWg6ZqclMHjekRX4vKBGISAKprKnbX5nvO5Pfd1a/r29+Z3nNQdtlpyWHKvbcTM4d3DV0Ft8xfBYfXt4pKxWz5lXysdh3HUB3DYmIHIJzjj1VdWzdU7W/kj+4b76KPYG6g7bNy0qlZ7hSPzk/b38XTc/cAxV9h4xUH35V2LTBTKzYwUSADCAAvAC82h0mr2uRXSgRiEibVl/vKKmoieiLr2rQVbOv6yay6wTALHTRtVduBv27ZDFqUGd65mbSMzednh0z91fyGanJPv2yRpyDYA3UVUOwFoLVofmKHdHLN7X8CCgRiIhv6oL17Nhb3ehia8NbKLfvCVAbbNgfn5Jk9AiftR/fuyNjh3YP98XvO4vPpHuHdFKTGw2e4FxEJVsNVXugPFzx1oUr3miVcV14ebA6YjqybM3B5Q71ndHK1de24pFvSIlARA6rvt5RVRuksiZIVU2Qytq6A9M1QSpr6vZPh8o1XF9dXU19zV5cdQXUVGA1lSTVlhMIVJFKHanUkUYtaVZHVlKQIZkwKgM650Bup3o6pjk6ptaTnVJPdnKQDAtikZXxzmrYfohKNrJsizJISYfkdEhODU+nhuZT0iA5LTydARm54fm0huWS08JlI78jotzz17dwzAdTIhBpAV4PARCL2mB9ROVbF1EpB6mKmD9QeR9YXhleHqiuxlVX4GoqsJoKkuoqsNpKUoKVZFNNlgXIoppsAmRa6G8WAbItQFeqD5SxanII/c0iQCoH980DkNbEj6kJf/ZJTm+6kt1XeaakQXqHRpVs6iEq6miVcWRF3cR85L6TkkN9UF5SIhBp+wJ/GMTE6pKDLuYF5nYhY8rG/eWcc1TX1YfPqCMq4cZn1rUHV9T714e3C1TX4GoqoaYCaitIrq0grf5AxZttoQo6K1wxZ1NNJgFyrJruBMgmQLZVk5MUKp9l1WS6AGlE6Z5IDn8acZZEMCULl5qNS8uG1GyS0jti6b1JSs/G0nIgLTv0Sc0+ML1/WVboTHn/2XCjynhfBZyU4n1lm+CUCESOQGVNHVt2V7GltIox1SVRy2RUlzDmrteor6nEaitJqiknkwCZVIcr6gOVcDaBBmfbPcPTHZKqybHq/WUyCYQr7EZdHIe4qcVh1Kdm41KzIS0LS8sJVdj7K+UcSMuKmA5X0vsr8qyoyy0lnRRV0N7L7h79wnB29xbbhRKBSCP7hgbYUlq1v7KPnN6+u5yMwDb6WjF9bQdjDlEJL6r8SmjCgPTD77s+NSt09pyeg4UrbdK6N31Wva/yTs2KXqmnZmGpmSSrwm6/WugW0UNRIpCEUxesZ9uewIFKfncVRWVVFIbnt5ZW0KF2N31tx/7K/qSUnVyaUkI+xXR2xSSnHzw+TFTn33HIs+oGn5RMkpL0ihBpfUoEEnciu20iz+SLwtPb9lTR0ZWTb8X7K/rT0nbxtZSd9GEHXVO2k5LcsOvF5fTA8vpDp9GQ1x869T/w98+nNB3MBVM8/rUiR0+JQNqVQ3XbFJWF/u6urCWTwP5Kvn/STs5O38WgcEXfJWsb6cGKhl+cnheu3IdHVPIDQn/z+mKpLTfAl0hbo0QgbUrjbpuicEVfGDEfqK0nlTr6hM/oj0nZyQUZuxmUUkLvjO10Sd5GVu3uhl+clAUd+0OnwZB34cFn9Rm5Rx50K1zME/GSEoG0qsqaOooiKvYtuw9U9qFumwD1DpKopwehfvrjM3ZzScYuBqbspHfuDjrXbiW7egdGxNOmdamQnR+u3EcefFaf3dW7WxBb4WKeiJeUCKTFRHbbHFTZR3TbhEvThT30T97JiVm7GZmxi4GZO+mdEa7oA9tI2vfIvQOqDDr2DlXwncY2PKPP6xdal9RGxowRaWeUCCRmh+q2KSqtoqi04cBfHahkcFoJJ2WXMSZ9FwM676RX3na61G4lu6qI5LrKUMHa8Cera6hS7zSiUdfNAMjNDz1cJCItTolAogrUBpn1n09Zu23vQd02+6RTw4lZZZyUU8ol6bvo37uYXm4HnWu2kl1VSEp1WahgVfiT1iFUuXcbAp0uPvisPj3Hj58qkvCUCCSqmYs38ucFqxnesYKTckr5audd9O8aqug71Wwlu3ILKZXboR7YE94oOT1UoXftD3mjwpV8vwNn9ZmdNFSASBukRCAHKSmvZvHi1/go63dk15TDrvAKS4KO4Quy+Rc1vOsmrz/k9AA9ECXS7igRyEGmL9zAj91jZKSlwsV/OdB1k5sfGq1RROKKEoE0sHlXJWvencuvUj6C0b+D077ld0gi4jG146WBP80v4ObkpwhmdYczvu93OCLSCjxNBGY23swKzGy9md0RZX2umf3bzD40s5Vm9h0v45FDW711D8UfzeMMW0PymNtCA6WJSNzzLBGYWTIwHZgADAOuMrNhjYrdAKxyzp0CjAH+aGZNvbNIPHbXK6u5Le1p6jvmq0tIJIF42SIYCax3zm10ztUATwKXNSrjgA5mZkAOoftTmninnXhpycYSktfP4yQ2kDTmdj28JZJAvEwEfYDNEfOF4WWR7geOB4qAj4GbnHP1jb/IzCaZ2TIzW1ZcXOxVvAnLOcedc1Zxe/oz1HcaBKdc7XdIItKKvEwE0Z4cco3mxwErgN7AqcD9ZtbxoI2cm+mcG+GcG9GtW7eWjzTBzVu5nT5F8xjsNpF0wc8gWTeTiSQSLxNBIdA3Yj6f0Jl/pO8As13IeuBTYKiHMUkjdcF67p67ktsyZuO6DYUTv+J3SCLSyrxMBEuBwWY2MHwB+ErgxUZlPgcuBDCzHsAQYKOHMUkjzywv5JRd8+lXvwUb+wuN4CmSgDzrA3DO1ZnZjcA8IBmY5ZxbaWbXhdfPAH4HPGxmHxPqSrrdObfTq5ikoaqaIPcvWMWzGc/hepyCDf2S3yGJiA887Qx2zs0B5jRaNiNiugi42MsYpGkPv/0Z51fOo0fqdhg7XQPCiSQoXRVMUKWVNcxatIr5GS9A71Fw7Bf8DklEfKJEkKAeXLSBS+vm0okSGPuIWgMiCUyJIAFtLavi6bfXsDj9Jeh3Pgwc7XdIIuIjDTqXgO5dsI5rbC4dgqUw9pd+hyMiPlOLIMGs276XucvXsCTrZThmPPQ9w++QRMRnahEkmGnzCrgubS6Zwb1wwc/9DkdE2gAlggSyfNMulq5ax/dSXoFhE6HXyX6HJCJtgBJBgnDOcecrBdycOYfU+gBc8DO/QxKRNkKJIEEsLNjBp59t4Gqbh510BXQb4ndIItJGKBEkgGB9qDUwJedlkgnCmNv9DklE2hAlggTw/AdbKN++kYnBBdjwa6DzIL9DEpE2RIkgzgVqg9yzYC2/zn0ZSzI4b7LfIYlIG6NEEOceXbKJ1LKNXFTzGjbiu5Cb73dIItLG6IGyOLYnUMv0heuZ3uklrDYdzv1vv0MSkTZILYI4NvONjXSr2shZlYtg5CTo0MPvkESkDVKLIE7t2BPg7299yhNdXsZqO8A5N/kdkoi0UWoRxKk/v7aO4+rXc2r5YjjrBsjq7HdIItJGqUUQhz7dWcGTSzfzStd/Q20nGHW93yGJSBumFkEcunt+AaNS1nLcniWhLqGMXL9DEpE2TC2COPNRYSkvf1TEm91fhGD30EViEZFDUIsgjjjnmPrKGsZnFdB3z3IYfQukZfsdloi0cWoRxJE31+3k7Q07ea/H8+D6wOn/5XdIItIOqEUQJ+rrHXfOXcMVHVfSvewjOP82SM3wOywRaQfUIogTL328lVVFpTzWYzZkD4BTv+l3SCLSTigRxIGaunrunlfA9zp/Ql7ZGrj8r5Cc6ndYItJOqGsoDjy59HMKd5Vzc8q/oOsQOOnrfockIu2IWgTtXEV1Hfe9to7/7rGC7LIN8PVHICnZ77BEpB1Ri6Cde+jNTyktr+QH9U9Dz5Pg+Ev9DklE2hm1CNqxkvJqZi7ewG/6riC9+HP48tOQpNwuIs2jWqMd+8vr6wnWVvGNyicg/wwYfLHfIYlIO6RE0E5t3lXJY+9uYtrAD0it2ApjfwFmfoclIu2QEkE7dc+CtWRbNZeUPg4DRsOgMX6HJCLtlK4RtEOrivbw/Iot/OPYpSRvLoaxj/odkoi0Y2oRtEN3zVtD7/Qazit+DI69CPqN8jskEWnHPE0EZjbezArMbL2Z3dFEmTFmtsLMVprZG17GEw/e2VDCooJi/jzgHZICpTD2536HJCLtnGddQ2aWDEwHLgIKgaVm9qJzblVEmTzgAWC8c+5zM+vuVTzxwDnH1LlrGNKxltOLHoPjvwy9h/sdloi0c162CEYC651zG51zNcCTwGWNylwNzHbOfQ7gnNvhYTzt3ryV2/hwcyn39l2M1VTABWoNiMjR8zIR9AE2R8wXhpdFOg7oZGaLzGy5mX0r2heZ2SQzW2Zmy4qLiz0Kt22rC9Zz17wCRnatZeimx0PjCXU/3u+wRCQOeJkIot3U7hrNpwCnA18ExgG/NLPjDtrIuZnOuRHOuRHdunVr+UjbgX8tL2RjcQXTer6KBWtgTNRLLiIizRZTIjCzZ83si2bWnMRRCPSNmM8HiqKUmeucq3DO7QQWA6c0Yx8JoaomyJ8WrOXi/Dr6ffoUnHo1dDnG77BEJE7EWrE/SKg/f52ZTTWzoTFssxQYbGYDzSwNuBJ4sVGZF4DRZpZiZlnAmcDqGGNKGP94+1N27K3mfzq/EmpmnX+73yGJSByJ6a4h59yrwKtmlgtcBSwws83A34BHnXO1UbapM7MbgXlAMjDLObfSzK4Lr5/hnFttZnOBj4B64CHn3Cct8sviRGllDQ8u2sCVx9TRff2/YMR3Ia/v4TcUEYlRzLePmlkX4BrgWuAD4DHgXODbwJho2zjn5gBzGi2b0Wh+GjCtOUEnkgcWbaC8uo4p2S9AUiqMvsXvkEQkzsSUCMxsNjAU+D/gy865reFVT5nZMq+CS3RFpVU8/PZnXD+sjty1s+HsH0OHnn6HJSJxJtYWwf3OudejrXDOjWjBeCTCnxasBQc32tOQlg3n/NTvkEQkDsV6sfj48FPAAJhZJzP7kUcxCbBu+16efb+QyacEyFr/Eoz6EWR38TssEYlDsSaCHzjnSvfNOOd2Az/wJiQBuGteAdlpKXy7+nHIyIOzbvA7JBGJU7EmgiSzA289CY8jlOZNSLLss10sWLWdX59WSdqG+XDOTyAz7/AbiogcgVivEcwDnjazGYSeDr4OmOtZVAnMOcedc9fQrUM6l++eAdndYOQP/Q5LROJYrIngduCHwPWEho6YDzzkVVCJ7LXVO1j62W7+NrqS5KWLYdwfID3H77BEJI7F+kBZPaGnix/0NpzEFqx33DVvDQO7ZHHh1nuhQ+/QA2QiIh6K9TmCwcAfgGFAxr7lzrlBHsWVkJ77YAtrt5fz9Nhykt5+D754D6RmHH5DEZGjEOvF4n8Qag3UARcA/yT0cJm0kEBtkHvmF3BKn46c8el0yOsPw6/1OywRSQCxJoJM59xrgDnnNjnnfgOM9S6sxPPokk0UlQW484TPsa0fhoaZTtGNWSLivVgvFgfCQ1CvCw8ktwXQayVbyJ5ALfcvXM95x3Zi6OrfQZfBcNIVfoclIgki1hbBT4Es4CeEXiRzDaHB5qQF/PWNDZRW1vL7weugeDVc8DNI9ux10iIiDRy2tgk/PHaFc24yUA58x/OoEsiOPQH+/tanTDy5O30//AX0OBGGTfQ7LBFJIIdtETjngsDpkU8WS8u597V1BOsdv8z/EHZtDL2QPsnLN4iKiDQUa//DB8ALZvYvoGLfQufcbE+iShAbi8t5aulmvn1GT7osvw36nA5DJvgdlogkmFgTQWeghIZ3CjlAieAo3D2/gPSUJG7usgQ+3AyX3gdqeIlIK4v1yWJdF2hhH24uZc7H27hlTF86vPcT6H8ODLrA77BEJAHF+mTxPwi1ABpwzmn8gyPgnGPqK2vokp3GpKzXoXw7fP1htQZExBexdg29FDGdAVwOFLV8OIlh8bqdvLOxhN9P6Ef6O5PgmAuh/9l+hyUiCSrWrqFnI+fN7AngVU8iinP19Y47X1lD386ZfKN+DlTtgrE/9zssEUlgR3qf4mCgX0sGkij+/VERq7buYcqYHqQsuR+Gfil0t5CIiE9ivUawl4bXCLYRekeBNENNXT1/nL+WYb06MqHsGajeG3qKWETER7F2DXXwOpBE8MR7n/P5rkoeu2oQ9tIMOPEr0OMEv8MSkQQXU9eQmV1uZrkR83lmpnEQmqG8uo77XlvHWYO6cPbWf0JdAMaoNSAi/ov1GsGvnXNl+2acc6XAr70JKT499OZGSipq+MV5udjSv8MpV0PXY/0OS0Qk5kQQrZyGx4zRzvJq/rZ4I5ec1JMT1s8EVw/n3+Z3WCIiQOyJYJmZ3WNmx5jZIDP7E7Dcy8Diyf2vrydQV88dozLh/X/C6d+GTv39DktEBIg9EfwYqAGeAp4GqoAbvAoqnnxeUslj727iihF96ffRXyApBUbf6ndYIiL7xXrXUAVwh8exxKU/LiggOcm45TTgkSdh1I+gYy+/wxIR2S/Wu4YWmFlexHwnM5vnXVjx4ZMtZbywoojvnjOQrkvvgZRMOPdmv8MSEWkg1q6hruE7hQBwzu1G7yw+rLvmFZCXlcr1x1fBytkw6nrI7up3WCIiDcSaCOrNbP+QEmY2gCijkcoBb2/YyeK1xdww5lg6vH0XpOfC2Tf6HZaIyEFivQX058BbZvZGeP48YJI3IbV/zoUGluudm8G3+u2E1+fA2F9AZie/QxMROUhMLQLn3FxgBFBA6M6hWwjdOSRRvPLJNj4sLOPmi44jffH/QlYXOPM6v8MSEYkq1ovF3wdeI5QAbgH+D/hNDNuNN7MCM1tvZk3edWRmZ5hZ0My+FlvYbVdtsJ675xVwXI8cvtL5M9i4MHSBOF3DNYlI2xTrNYKbgDOATc65C4DhQPGhNjCzZGA6MAEYBlxlZsOaKHcnEBd3IT29bDMbd1Zw28VDSF70v5DTE874vt9hiYg0KdZEEHDOBQDMLN05twYYcphtRgLrnXMbnXM1wJPAZVHK/Rh4FtgRYyxtVlVNkD+/uo4R/TtxYdon8PnbcN6tkJrpd2giIk2K9WJxYfg5gueBBWa2m8O/qrIPsDnyO4AzIwuYWR9Cr70cS6jFEZWZTSJ8cbpfv7b7PpxZ//mUHXureeDq4diCr0JuPzjt236HJSJySLE+WXx5ePI3ZrYQyAXmHmazaG9ib3zL6b3A7c65oB3ixe3OuZnATIARI0a0ydtWd1fUMGPRBr5wfA9GVC+Bog/gsumQkuZ3aCIih9TsEUSdc28cvhQQagH0jZjP5+BWxAjgyXAS6ApcYmZ1zrnnmxuX3x5YtJ6KmjpuGzcYZv8EOh8DJ1/pd1giIofl5VDSS4HBZjYQ2AJcCVwdWcA5N3DftJk9DLzUHpPAltIqHnl7E189LZ/jihfAjpXw1b9DskbqFpG2z7OayjlXZ2Y3ErobKBmY5ZxbaWbXhdfP8Grfre1PC9aCwc0XDoLHvgvdh8EJX/E7LBGRmHh6yuqcmwPMabQsagJwzv2Xl7F4pWDbXma/X8j3zh1I700vQMl6+MZjkBTrDVkiIv5SbXWUps1bQ3Z6Cj8a3Q8W3Qm9h8PQL/odlohIzJQIjsLSz3bx6uodXHf+MXQqeBLKPg+NKXSIO6BERNoaJYIj5Jxj6itr6N4hne+O7AmL74Z+Z8ExF/odmohIsygRHKFXV+9g+abd/PQLx5H54cOwd6taAyLSLikRHIFgveOuuWsY1DWbK07Og7fugUEXwIBz/Q5NRKTZlAiOwLPvF7JuRzmTxw0hZelMqCwJtQZERNohJYJmCtQG+dOCtZzSN4/xx2bA2/fBcRMgf4TfoYmIHBElgmb65zufsbUswB3jh2LvTIdAGYz9ud9hiYgcMSWCZiirqmX6wg2cf1w3zurpYMmDcMLl0PMkv0MTETliSgTN8Nc3NlBWVctt44fAW3+C2koY8zO/wxIROSpKBDHavifArP98ysRTe3NCTiUsfSg0umi34/wOTUTkqCgRxOjeV9cRrHfccvEQePOPUF8H59/md1giIkdNiSAGG4rLeXrZZr55Zn/6WjEsfxiGXwudBx52WxGRtk6JIAZ3zysgIyWJG8ceC4vvAkuC8yb7HZaISItQIjiMDz7fzSufbOMH5w2ia2AzrHgCzvge5PbxOzQRkRahV2gdgnOOO+euoWtOGt8fPQj+/UNISYdzb/Y7NBGRFqMWwSG8sbaYJRt38eOxg8kpLYBPnoUzr4Oc7n6HJiLSYtQiaEJ9vePOuQX065zFVSP7wTPXQnoHOPvHfocmItKi1CJowosfFrF66x5uufg40ravgDUvwVk3QlZnv0MTEWlRSgRRVNcFuXt+ASf07siXT+4NC38PmZ1h1PV+hyYi0uKUCKJ4/N3PKdxdxe3jh5K0eQmsfxXO/SlkdPQ7NBGRFqdE0Eh5dR33v76es4/pwuhju8Dr/wM5PeCMH/gdmoiIJ5QIGvnb4o2UVNRw+/ih2KdvwKa3YPStkJbld2giIp5QIohQvLeav725kS+e1ItT8nNDrYGO+XD6t/0OTUTEM0oEEe5/fR3VdfXccvFxsHYubFkWGlguJd3v0EREPKNEELappILH3v2cK8/oy6AuWfD676HzIDj1ar9DExHxlB4oC/vj/LWkJidx04WDYfULsP1j+MrfIDnV79BERDylFgHwyZYyXvywiO+dO5DuOamw8H+h21A48at+hyYi4jm1CIA7566hU1Yqk84fBB89DTvXwhX/B0nJfocmIuK5hG8R/Gf9Tt5ct5MbLjiWjqnAoj9Az5Ph+C/7HZqISKtI6BbBvmGm++Rlcs2o/vDBI1C6Ca7+F5j5HZ6ISKtI6BbBnI+38VFhGTdfdBwZ1MIb06DvmTD4Ir9DExFpNQmbCGqD9dw9v4AhPTpw+fA+sPwfsLcIxv5CrQERSSgJmwieWrqZT3dWcNv4ISTXVcKbf4SB54U+IiIJxNNEYGbjzazAzNab2R1R1n/TzD4Kf942s1O8jGefypo6/vzaOs4Y0ImxQ7vDu3+FimIY+8vW2L2ISJviWSIws2RgOjABGAZcZWbDGhX7FDjfOXcy8DtgplfxRJr11qcU763mjglDseo98J8/w+Bx0Hdka+xeRKRN8bJFMBJY75zb6JyrAZ4ELoss4Jx72zm3Ozy7BMj3MB4AdlfU8Nc3NnLRsB6c3r8zvDMdAqUw9ude71pEpE3yMhH0ATZHzBeGlzXle8Ar0VaY2SQzW2Zmy4qLi48qqOkL11NRU8dt44ZARQm88wAMuwx6tUqvlIhIm+PlcwTRbr1xUQuaXUAoEZwbbb1zbibhbqMRI0ZE/Y5Def6DLUybV0BRaRUOOHNgJwb36AALfgU15TDmZ839ShGRuOFli6AQ6Bsxnw8UNS5kZicDDwGXOedKWjqI5z/YwpTZH7MlnAQAVmwu45V3VsC7M+HkK6D70JberYhIu+FlIlgKDDazgWaWBlwJvBhZwMz6AbOBa51za70IYtq8Aqpqgw2WVdfVU/7qXRCsgTEH3cwkIpJQPOsacs7VmdmNwDwgGZjlnFtpZteF188AfgV0AR6w0ENcdc65ES0ZR1Fp1UHLerOTS+vmwenXhN45ICKSwDwda8g5NweY02jZjIjp7wPf9zKG3nmZbGmUDH6c8lzo4eHzJnu5axGRdiHuB52bPG4IU2Z/vL97qL9t4+vJb7Bp0NUck9f3MFuLSLyora2lsLCQQCDgdyieysjIID8/n9TU2F+qFfeJYOLw0B2r++4ampL5Algax1z+K58jE5HWVFhYSIcOHRgwYAAWp+OJOecoKSmhsLCQgQMHxrxd3CcCpg1mYsUOJgJkAPXh5TPOhcnr/ItLRFpVIBCI64VKNuEAAAscSURBVCQAYGZ06dKF5j5vFf+DzlXsaN5yEYlb8ZwE9jmS3xj/iUBERA5JiUBEJIrnP9jCOVNfZ+AdL3PO1Nd5/oMtR/V9paWlPPDAA83e7pJLLqG0tPSo9n04SgQiIo00HpFgS2kVU2Z/fFTJoKlEEAwGo5Q+YM6cOeTl5R3xfmMR/xeLRUQa+e2/V7KqaE+T6z/4vJSaYH2DZVW1QW575iOeeO/zqNsM692RX3/5hCa/84477mDDhg2ceuqppKamkpOTQ69evVixYgWrVq1i4sSJbN68mUAgwE033cSkSZMAGDBgAMuWLaO8vJwJEyZw7rnn8vbbb9OnTx9eeOEFMjMzj+AINBT/LYLs7s1bLiIJr3ESONzyWEydOpVjjjmGFStWMG3aNN577z1+//vfs2rVKgBmzZrF8uXLWbZsGffddx8lJQcPvbZu3TpuuOEGVq5cSV5eHs8+++wRxxMp/lsEukVURBo51Jk7wDlTXz9oRAKAPnmZPPXDs1okhpEjRza41/++++7jueeeA2Dz5s2sW7eOLl26NNhm4MCBnHrqqQCcfvrpfPbZZy0SS/y3CEREmmnyuCFkpiY3WJaZmszkcUNabB/Z2dn7pxctWsSrr77KO++8w4cffsjw4cOjPgGdnp6+fzo5OZm6uroWiSX+WwQiIs3UeESC3nmZTB43ZP/yI9GhQwf27t0bdV1ZWRmdOnUiKyuLNWvWsGTJkiPez5FQIhARiWLi8D5HVfE31qVLF8455xxOPPFEMjMz6dGjx/5148ePZ8aMGZx88skMGTKEUaNGtdh+Y2HONfuFX74aMWKEW7Zsmd9hiEg7s3r1ao4//ni/w2gV0X6rmS1vaph/XSMQEUlwSgQiIglOiUBEJMEpEYiIJDglAhGRBKdEICKS4PQcgYhIY9MGR395VXb3Ix62prS0lMcff5wf/ehHzd723nvvZdKkSWRlZR3Rvg9HLQIRkcY8eLPhkb6PAEKJoLKy8oj3fThqEYhI4nnlDtj28ZFt+48vRl/e8ySYMLXJzSKHob7ooovo3r07Tz/9NNXV1Vx++eX89re/paKigiuuuILCwkKCwSC//OUv2b59O0VFRVxwwQV07dqVhQsXHlnch6BEICLSCqZOnconn3zCihUrmD9/Ps888wzvvfcezjkuvfRSFi9eTHFxMb179+bll18GQmMQ5ebmcs8997Bw4UK6du3qSWxKBCKSeA5x5g7Ab3KbXvedl4969/Pnz2f+/PkMHz4cgPLyctatW8fo0aO59dZbuf322/nSl77E6NGjj3pfsVAiEBFpZc45pkyZwg9/+MOD1i1fvpw5c+YwZcoULr74Yn71q195Ho8uFouINObBmw0jh6EeN24cs2bNory8HIAtW7awY8cOioqKyMrK4pprruHWW2/l/fffP2hbL6hFICLSmAdvNowchnrChAlcffXVnHVW6G1nOTk5PProo6xfv57JkyeTlJREamoqDz74IACTJk1iwoQJ9OrVy5OLxRqGWkQSgoah1jDUIiLSBCUCEZEEp0QgIgmjvXWFH4kj+Y1KBCKSEDIyMigpKYnrZOCco6SkhIyMjGZtp7uGRCQh5OfnU1hYSHFxsd+heCojI4P8/PxmbaNEICIJITU1lYEDB/odRpvkadeQmY03swIzW29md0RZb2Z2X3j9R2Z2mpfxiIjIwTxLBGaWDEwHJgDDgKvMbFijYhOAweHPJOBBr+IREZHovGwRjATWO+c2OudqgCeByxqVuQz4pwtZAuSZWS8PYxIRkUa8vEbQB9gcMV8InBlDmT7A1shCZjaJUIsBoNzMCo4wpq7AziPc1kttNS5ou7EpruZRXM0Tj3H1b2qFl4nAoixrfN9WLGVwzs0EZh51QGbLmnrE2k9tNS5ou7EpruZRXM2TaHF52TVUCPSNmM8Hio6gjIiIeMjLRLAUGGxmA80sDbgSeLFRmReBb4XvHhoFlDnntjb+IhER8Y5nXUPOuTozuxGYByQDs5xzK83suvD6GcAc4BJgPVAJfMereMKOunvJI201Lmi7sSmu5lFczZNQcbW7YahFRKRlaawhEZEEp0QgIpLg4jIRtNWhLWKIa4yZlZnZivDH+7dWh/Y7y8x2mNknTaz363gdLq5WP15m1tfMFprZajNbaWY3RSnT6scrxrj8OF4ZZvaemX0Yjuu3Ucr4cbxiicuXf4/hfSeb2Qdm9lKUdS1/vJxzcfUhdGF6AzAISAM+BIY1KnMJ8Aqh5xhGAe+2kbjGAC/5cMzOA04DPmlifasfrxjjavXjBfQCTgtPdwDWtpH/v2KJy4/jZUBOeDoVeBcY1QaOVyxx+fLvMbzv/wYej7Z/L45XPLYI2urQFrHE5Qvn3GJg1yGK+DIUSAxxtTrn3Fbn3Pvh6b3AakJPw0dq9eMVY1ytLnwMysOzqeFP4ztU/DhescTlCzPLB74IPNREkRY/XvGYCJoatqK5ZfyIC+CscHP1FTM7weOYYuXH8YqVb8fLzAYAwwmdTUby9XgdIi7w4XiFuzlWADuABc65NnG8YogL/Pn/617gNqC+ifUtfrziMRG02NAWLSyWfb4P9HfOnQL8BXje45hi5cfxioVvx8vMcoBngZ865/Y0Xh1lk1Y5XoeJy5fj5ZwLOudOJTRywEgzO7FREV+OVwxxtfrxMrMvATucc8sPVSzKsqM6XvGYCNrq0BaH3adzbs++5qpzbg6QamZdPY4rFm1yKBC/jpeZpRKqbB9zzs2OUsSX43W4uPz+/8s5VwosAsY3WuXr/19NxeXT8ToHuNTMPiPUfTzWzB5tVKbFj1c8JoK2OrTFYeMys55mZuHpkYT++5R4HFcs2uRQIH4cr/D+/g6sds7d00SxVj9escTl0/HqZmZ54elM4AvAmkbF/Dheh43Lj+PlnJvinMt3zg0gVEe87py7plGxFj9ecfeqStc2h7aINa6vAdebWR1QBVzpwrcJeMnMniB0h0RXMysEfk3o4plvxyvGuPw4XucA1wIfh/uXAX4G9IuIy4/jFUtcfhyvXsAjFnpRVRLwtHPuJb//PcYYly//HqPx+nhpiAkRkQQXj11DIiLSDEoEIiIJTolARCTBKRGIiCQ4JQIRkQSnRCDiMQuNYnnQKJIibYUSgYhIglMiEAkzs2ssNEb9CjP7a3hQsnIz+6OZvW9mr5lZt3DZU81siYXGg3/OzDqFlx9rZq+GByp738yOCX99jpk9Y2ZrzOyxiCdWp5rZqvD33O3TT5cEp0QgApjZ8cA3gHPCA5EFgW8C2cD7zrnTgDcIPd0M8E/gdufcycDHEcsfA6aHByo7G9j36P9w4KfAMELvpDjHzDoDlwMnhL/nf7z9lSLRKRGIhFwInA4sDQ/RcCGhCrseeCpc5lHgXDPLBfKcc2+Elz8CnGdmHYA+zrnnAJxzAedcZbjMe865QudcPbACGADsAQLAQ2b2FULDBYi0OiUCkRADHnHOnRr+DHHO/SZKuUONyRJteOB9qiOmg0CKc66O0AuLngUmAnObGbNIi1AiEAl5DfiamXUHMLPOZtaf0L+Rr4XLXA285ZwrA3ab2ejw8muBN8Lj/xea2cTwd6SbWVZTO7TQuwNyw0Mc/xQ41YsfJnI4cTf6qMiRcM6tMrNfAPPNLAmoBW4AKoATzGw5UEboOgLAt4EZ4Yp+IwdGgLwW+KuZ/b/wd3z9ELvtALxgZhmEWhM3t/DPEomJRh8VOQQzK3fO5fgdh4iX1DUkIpLg1CIQEUlwahGIiCQ4JQIRkQSnRCAikuCUCEREEpwSgYhIgvv/7qVlIdFjxHkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感想\n",
    "グラフから正答率が正しく上昇していることが確かめられた。まだ理解できていないところを理解できるようにしたい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "【ゼロから作るDeep Learning】"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
